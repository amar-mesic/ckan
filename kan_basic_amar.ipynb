{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first attempt to recreate a KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from spline import *\n",
    "from utils import *\n",
    "from scipy.interpolate import BSpline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAN Layer implementation based off of PyKAN\n",
    "class KANLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, num, k=3, noise_scale=0.1, scale_base=1.0, grid_range=[-1, 1], device='cpu'):\n",
    "        super(KANLayer, self).__init__()\n",
    "\n",
    "        # self.func_matrix = nn.Parameter(torch.zeros(in_dim, out_dim))\n",
    "\n",
    "        # square = lambda x: x**2\n",
    "\n",
    "        # self.func_matrix[0,0] = square\n",
    "\n",
    "        self.size = size = out_dim * in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim = in_dim\n",
    "        self.num = num\n",
    "        self.k = k\n",
    "\n",
    "        self.grid = torch.linspace(grid_range[0], grid_range[1], steps=num + 1, device=device).repeat(size, 1)\n",
    "        self.grid = torch.nn.Parameter(self.grid).requires_grad_(False)\n",
    "\n",
    "        # NO IDEA WHAT THIS DOES\n",
    "        noises = (torch.rand(size, self.grid.shape[1]) - 1 / 2) * noise_scale / num\n",
    "        noises = noises.to(device)\n",
    "\n",
    "        self.coef = torch.nn.Parameter(curve2coef(self.grid, noises, self.grid, k, device))\n",
    "\n",
    "        # NOR THIS\n",
    "        # if isinstance(scale_base, float):\n",
    "        #     self.scale_base = torch.nn.Parameter(torch.ones(size, device=device) * scale_base).requires_grad_(sb_trainable) \n",
    "\n",
    "\n",
    "        self.mask = torch.nn.Parameter(torch.ones(size, device=device)).requires_grad_(False)\n",
    "        # self.grid_eps = grid_eps\n",
    "        self.weight_sharing = torch.arange(size)\n",
    "        self.lock_counter = 0\n",
    "        self.lock_id = torch.zeros(size)\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.shape[0]\n",
    "        # x: shape (batch, in_dim) => shape (size, batch) (size = out_dim * in_dim)\n",
    "        # x = torch.einsum('ij,k->ikj', x, torch.ones(self.out_dim, device=self.device)).reshape(batch, self.size).permute(1, 0)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only work from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset f(x,y) = exp(sin(pi*x)+y^2)\n",
    "f = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2)\n",
    "dataset = create_dataset(f, n_var=2)\n",
    "dataset['train_input'].shape, dataset['train_label'].shape, dataset['test_input'].shape, dataset['test_label'].shape\n",
    "\n",
    "X_train, y_train, X_test, y_test = dataset['train_input'], dataset['train_label'], dataset['test_input'], dataset['test_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KAN layer from scratch\n",
    "class MyKANLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, grid, degree=3, noise_scale=0.1, scale_base=1.0, grid_range=[-1, 1], device='cpu'):\n",
    "        super(MyKANLayer, self).__init__()\n",
    "\n",
    "        # initiliaze variables about the layer\n",
    "        self.size = size = out_dim * in_dim\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.degree = degree\n",
    "        self.grid_range = grid_range\n",
    "        self.cache = None\n",
    "\n",
    "        # The spline function requires three parameters: knots, coeff, and degree\n",
    "        # knots: the grid points for the spline\n",
    "        # self.knots = nn.Parameter(torch.linspace(grid_range[0], grid_range[1], steps=grid + 1, device=device).repeat(size, 1)).requires_grad_(False)\n",
    "        self.knots = torch.linspace(grid_range[0], grid_range[1], steps=grid + 1, device=device)\n",
    "\n",
    "        # coeff: the coefficients for the spline - THESE ARE LEARNABLE\n",
    "        # I am wrapping them in a parameter since that is what they are\n",
    "        self.coeff = nn.Parameter(torch.rand(size, grid + 1 + degree, device=device)).requires_grad_(True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # we process data in batches!\n",
    "        print('batch size: ', x.shape[0])\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # we need to repeat the input for each spline function\n",
    "        # x.shape = (size, batch_size)\n",
    "        x = x.transpose(0, 1).repeat(self.out_dim, 1)\n",
    "\n",
    "        # store the input for later\n",
    "        self.cache = x\n",
    "\n",
    "        # store the output of the spline functions\n",
    "        out = torch.zeros(self.size, batch_size)\n",
    "\n",
    "\n",
    "        # OLD WAY: WITH BUILT-IN B-SPLINE FUNCTION, WHICH DOES NOT PROVIDE US WITH GRADIENTS\n",
    "        # for i in range(self.size):\n",
    "        #     # x[i].shape = batch_size\n",
    "        #     spline = BSpline(self.knots[i], self.coeff[i].detach().numpy(), self.degree)\n",
    "        #     out[i] = torch.tensor(spline(x[i]))\n",
    "\n",
    "\n",
    "        # loop through all the spline functions, and apply them to a single element for the whole batch\n",
    "        # TODO: see if we can vectorize this\n",
    "        for i in range(self.size):\n",
    "            # Use torch operations to evaluate the B-spline\n",
    "            knots = self.knots\n",
    "            print('knots shape: ', knots.shape)\n",
    "            coeff = self.coeff[i]\n",
    "            print('coeff shape: ', coeff.shape)\n",
    "            spline_values = self.evaluate_spline(x[i], knots, coeff, self.degree)\n",
    "            out[i] = spline_values\n",
    "\n",
    "        \n",
    "        # plot the spline functions (optional)\n",
    "        # self.plot_splines()\n",
    "\n",
    "\n",
    "        # reshape the output to be of shape (out_dim, in_dim, batch_size)\n",
    "        # then we sum it as part of the algorithm\n",
    "        # then we transpose it so subsequent layers can use it\n",
    "        y = out.reshape(self.out_dim, self.in_dim, batch_size).sum(dim=1).transpose(0, 1)\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_spline(self, x, knots, coeff, degree):\n",
    "        # Implement the B-spline evaluation directly in PyTorch\n",
    "        # This is a simplified version and assumes a cubic B-spline (degree=3)\n",
    "        assert degree == 3, \"This implementation only supports cubic B-splines (degree=3)\"\n",
    "\n",
    "        # Initialize the B-spline basis functions\n",
    "        n_knots = len(knots)\n",
    "        n_coeffs = len(coeff)\n",
    "        # assert n_knots == n_coeffs + degree + 1, \"Mismatch between number of knots and coefficients for cubic B-splines\"\n",
    "\n",
    "        # Implement basis function evaluation (recursively or using a loop)\n",
    "        # This example uses a loop for simplicity\n",
    "        B = torch.zeros(x.shape[0], n_coeffs)\n",
    "\n",
    "        # Basis function calculation\n",
    "        for i in range(n_coeffs):\n",
    "            B[:, i] = self.basis_function(x, knots, i, degree)\n",
    "\n",
    "        # Evaluate the spline\n",
    "        spline_values = B.matmul(coeff)\n",
    "\n",
    "        return spline_values\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def basis_function(self, x, knots, degree):\n",
    "        # Compute the B-spline basis function value\n",
    "        # This is a placeholder for the basis function calculation\n",
    "        pass\n",
    "\n",
    "        # You need to implement the Cox-de Boor recursion formula here\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # pytorch may automatically handle this for us!!!\n",
    "    # def backward(self, dupstream):\n",
    "    #     pass\n",
    "\n",
    "\n",
    "    # If we want to plot the spline curves of a layer\n",
    "    def plot_splines(self):\n",
    "        # Plot the spline functions (optional)\n",
    "        points = np.linspace(self.grid_range[0], self.grid_range[1], 100)\n",
    "        for i in range(self.size):\n",
    "            spline = BSpline(self.knots[i].cpu().numpy(), self.coeff[i].detach().cpu().numpy(), self.degree)\n",
    "            y = spline(points)\n",
    "            plt.plot(points, y, label=f'B_{i,3}(points)')\n",
    "            \n",
    "        plt.title('Cubic B-spline Basis Functions')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('B_{i,3}(points)')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  2\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n",
      "knots shape:  torch.Size([9])\n",
      "coeff shape:  torch.Size([9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 9])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the layer\n",
    "layer = MyKANLayer(in_dim=2, out_dim=5, grid=8, degree=3)\n",
    "\n",
    "out = layer(dataset['train_input'][:2])\n",
    "out\n",
    "layer.coeff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKAN(nn.Module):\n",
    "    \n",
    "    def __init__(self, width=None, grid=3, degree=3, seed=69, device='cpu'):\n",
    "        super(MyKAN, self).__init__()\n",
    "\n",
    "        # intialize variables for the KAN\n",
    "        self.biases = []\n",
    "        self.act_fun = nn.ModuleList()\n",
    "        self.depth = len(width) - 1\n",
    "        self.width = width\n",
    "\n",
    "        # create the layers here\n",
    "        for l in range(self.depth):\n",
    "            kan_layer = MyKANLayer(width[l], width[l+1], grid, degree, device=device)\n",
    "            self.act_fun.append(kan_layer)\n",
    "\n",
    "    # x should only be passed in batches\n",
    "    def forward(self, x):\n",
    "        for l in range(self.depth):\n",
    "            x = self.act_fun[l](x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "\n",
    "    # # The backward and optimizer_step functions may not be necessary\n",
    "    # def backward(self, dupstream):\n",
    "    #     \"\"\"\n",
    "    #     Performs backward pass through all layers of the network.\n",
    "\n",
    "    #     Args:\n",
    "    #         dupstream: Gradient of loss with respect to output.\n",
    "    #     \"\"\"\n",
    "    #     dx = dupstream\n",
    "\n",
    "    #     for layer in reversed(self.layers):\n",
    "    #         dx = layer.backward(dx)\n",
    "\n",
    "    #     return dx\n",
    "    \n",
    "\n",
    "    # def optimizer_step(self, lr):\n",
    "    #     \"\"\"\n",
    "    #     Updates network weights by performing a step in the negative gradient\n",
    "    #     direction in each layer. The step size is determined by the learning\n",
    "    #     rate.\n",
    "\n",
    "    #     Args:\n",
    "    #         lr: Learning rate to use for update step.\n",
    "    #     \"\"\"\n",
    "    #     for layer in self.layers:\n",
    "    #         if hasattr(layer, 'weight'):\n",
    "    #             layer.weight -= layer.weight_grad * lr\n",
    "    #         if hasattr(layer, 'bias'):\n",
    "    #             layer.bias -= layer.bias_grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  10\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatch between number of knots and coefficients for cubic B-splines",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[189], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test the KAN here\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MyKAN(width\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m], grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m out\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[188], line 20\u001b[0m, in \u001b[0;36mMyKAN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[0;32m---> 20\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[186], line 53\u001b[0m, in \u001b[0;36mMyKANLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     knots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknots[i]\n\u001b[1;32m     52\u001b[0m     coeff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoeff[i]\n\u001b[0;32m---> 53\u001b[0m     spline_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_spline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m spline_values\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# plot the spline functions (optional)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# self.plot_splines()\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# then we sum it as part of the algorithm\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# then we transpose it so subsequent layers can use it\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[186], line 82\u001b[0m, in \u001b[0;36mMyKANLayer.evaluate_spline\u001b[0;34m(self, x, knots, coeff, degree)\u001b[0m\n\u001b[1;32m     80\u001b[0m n_knots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(knots)\n\u001b[1;32m     81\u001b[0m n_coeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(coeff)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m n_knots \u001b[38;5;241m==\u001b[39m n_coeffs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between number of knots and coefficients for cubic B-splines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Implement basis function evaluation (recursively or using a loop)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# This example uses a loop for simplicity\u001b[39;00m\n\u001b[1;32m     86\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x), n_coeffs, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Mismatch between number of knots and coefficients for cubic B-splines"
     ]
    }
   ],
   "source": [
    "# test the KAN here\n",
    "model = MyKAN(width=[2, 5, 1], grid=10, degree=3)\n",
    "\n",
    "out = model(dataset['train_input'][:10])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  1000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatch between number of knots and coefficients for cubic B-splines",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)  \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass (compute gradients)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[188], line 20\u001b[0m, in \u001b[0;36mMyKAN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[0;32m---> 20\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/cv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[186], line 53\u001b[0m, in \u001b[0;36mMyKANLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     knots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknots[i]\n\u001b[1;32m     52\u001b[0m     coeff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoeff[i]\n\u001b[0;32m---> 53\u001b[0m     spline_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_spline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m spline_values\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# plot the spline functions (optional)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# self.plot_splines()\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# then we sum it as part of the algorithm\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# then we transpose it so subsequent layers can use it\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[186], line 82\u001b[0m, in \u001b[0;36mMyKANLayer.evaluate_spline\u001b[0;34m(self, x, knots, coeff, degree)\u001b[0m\n\u001b[1;32m     80\u001b[0m n_knots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(knots)\n\u001b[1;32m     81\u001b[0m n_coeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(coeff)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m n_knots \u001b[38;5;241m==\u001b[39m n_coeffs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between number of knots and coefficients for cubic B-splines\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Implement basis function evaluation (recursively or using a loop)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# This example uses a loop for simplicity\u001b[39;00m\n\u001b[1;32m     86\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x), n_coeffs, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Mismatch between number of knots and coefficients for cubic B-splines"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# stick to MSE Loss for now, later we can do cross entropy\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "    outputs = model(X_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Compute the loss\n",
    "\n",
    "    loss.backward()  # Backward pass (compute gradients)\n",
    "    optimizer.step()  # Update the parameters\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, you can evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_train)  # Forward pass on the training data\n",
    "    test_loss = criterion(test_outputs, y_train)  # Compute the loss\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
