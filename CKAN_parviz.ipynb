{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aabded9-bb76-4929-a0ea-e52e5ca7dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "import spline\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92b794-abb7-4476-9771-e48c2091b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is just a copy of Conv implementation from the deep learning lab, wanted to test how fast it trained when using custom implementation\n",
    "class Conv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    2D convolutional layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(Conv2dd, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the layer with given params\n",
    "\n",
    "        Args:\n",
    "            in_channels: # channels that the input has.\n",
    "            out_channels: # channels that the output will have.\n",
    "            kernel_size: height and width of the kernel in pixels.\n",
    "            stride: # pixels between adjacent receptive fields in both\n",
    "                horizontal and vertical.\n",
    "            padding: # pixels that is used to zero-pad the input.\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        ########################################################################\n",
    "        #        TODO: Create place holder tensors for weight and bias         #\n",
    "        #                       with correct dimensions.                       #\n",
    "        ########################################################################\n",
    "\n",
    "        self.weight = torch.Tensor(out_channels,\n",
    "                                   in_channels,\n",
    "                                   kernel_size,\n",
    "                                   kernel_size)\n",
    "        self.bias = torch.Tensor(out_channels)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def init_params(self, std=0.7071):\n",
    "        \"\"\"\n",
    "        Initialize layer parameters. Sample weight from Gaussian distribution\n",
    "        and bias will be zeros.\n",
    "\n",
    "        Args:\n",
    "            std: Standard deviation of Gaussian distribution (default: 0.7071)\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight = std * torch.randn_like(self.weight)\n",
    "        self.bias = torch.rand_like(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate output dimensions\n",
    "        N, _, H, W = x.shape\n",
    "        Hp = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        Wp = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "\n",
    "        # Use unfold to extract sliding windows\n",
    "        x_unfolded = F.unfold(x, kernel_size=self.kernel_size, dilation=1, padding=self.padding, stride=self.stride)\n",
    "        x_unfolded = x_unfolded.transpose(1, 2).reshape(N, Hp * Wp, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        # Perform the custom operation for each output channel\n",
    "        output = torch.zeros(N, self.out_channels, Hp, Wp, device=x.device)\n",
    "        for c in range(self.out_channels):\n",
    "            # Apply your custom function - modify as necessary to use batch processing\n",
    "            temp = x_unfolded.reshape(N * Hp * Wp, -1).t()  # Flatten and transpose for batch processing\n",
    "            spline_values = spline.coef2curve(temp, self.knots[c], self.coeff[c], self.degree)\n",
    "            output[:, c, :, :] = spline_values.sum(dim=0).view(N, Hp, Wp)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dupstream):\n",
    "        \"\"\"\n",
    "        Backward pass of convolutional layer: calculate gradients of loss with\n",
    "        respect to weight and bias and return downstream gradient dx.\n",
    "\n",
    "        Args:\n",
    "            dupstream: Gradient of loss with respect to output of this layer.\n",
    "\n",
    "        Returns:\n",
    "            dx: Gradient of loss with respect to input of this layer.\n",
    "        \"\"\"\n",
    "\n",
    "        # You don't need to implement the backward pass. Instead we give it to\n",
    "        # you the solution.\n",
    "\n",
    "        # Unpack cache\n",
    "        x_padded = self.cache\n",
    "\n",
    "        # Create an empty dx tensor to accumulate the gradients on. Keep in mind\n",
    "        # that it has a size according to padded input\n",
    "        dx_padded = torch.zeros_like(x_padded)\n",
    "\n",
    "        # Also initialize the weight gradients as zeros\n",
    "        self.weight_grad = torch.zeros_like(self.weight)\n",
    "\n",
    "        # Unpack needed dimensions\n",
    "        N, _, Hp, Wp = dupstream.shape\n",
    "\n",
    "        # Loop through dupstream\n",
    "        for i in range(Hp):\n",
    "            for j in range(Wp):\n",
    "\n",
    "                # Calculate offset for current window on input\n",
    "                h_offset = i * self.stride\n",
    "                w_offset = j * self.stride\n",
    "\n",
    "                # Get current window of input and gradient of the input\n",
    "                window = x_padded[:, :, h_offset:h_offset+self.kernel_size, w_offset:w_offset+self.kernel_size]\n",
    "                dwindow = dx_padded[:, :, h_offset:h_offset+self.kernel_size, w_offset:w_offset+self.kernel_size]\n",
    "\n",
    "                # Walk through each sample of the input and accumulate gradients\n",
    "                # of both input and weight\n",
    "                for k in range(N):\n",
    "                    dwindow[k] += (self.weight * dupstream[k, :, i, j].view(-1, 1, 1, 1)).sum(dim=0)\n",
    "                    self.weight_grad += window[k].view(1, self.in_channels, self.kernel_size, self.kernel_size) * dupstream[k, :, i, j].view(-1, 1, 1, 1)\n",
    "        # Calculate actual size of input height and width\n",
    "        H = x_padded.shape[2] - 2 * self.padding\n",
    "        W = x_padded.shape[3] - 2 * self.padding\n",
    "\n",
    "        # Unpad dx\n",
    "        dx = dx_padded[:, :, self.padding:self.padding+H, self.padding:self.padding+W]\n",
    "\n",
    "        # Calculate bias gradients\n",
    "        self.bias_grad = dupstream.sum(dim=(0, 2, 3))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a970b4c0-c364-48a8-9788-b9a7e4acee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Main implementation of CKAN\n",
    "class CKAN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, grid, stride=1, padding=0, degree=3, grid_range=[-0.5, 1], device='cpu'):\n",
    "        super(CKAN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.degree = degree\n",
    "        self.grid_range = grid_range\n",
    "        self.device = device\n",
    "        \n",
    "        # Knots will have shape (out_channels,  in_channels * kernel_size * kernel_size, grid+1)\n",
    "        knots = torch.linspace(grid_range[0], grid_range[1], steps=grid + 1, device=device).view(1, 1, -1)\n",
    "        knots = knots.repeat(out_channels, in_channels * kernel_size * kernel_size, 1)\n",
    "        self.knots = nn.Parameter(knots, requires_grad=False)\n",
    "        # Coeff will have shape (out_channels,  in_channels * kernel_size * kernel_size, grid+degree)\n",
    "        self.coeff = nn.Parameter(0.1 * torch.randn(out_channels, in_channels * kernel_size * kernel_size, grid + degree, device=device), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, _, H, W = x.shape\n",
    "        x_padded = F.pad(x, [self.padding, self.padding, self.padding, self.padding])\n",
    "\n",
    "        # Unfold to get all sliding windows - Shape becomes  (N, C*K*K, L) where L is the number of extracted windows\n",
    "        unfolded = F.unfold(x_padded, kernel_size=self.kernel_size, stride=self.stride, padding=0)\n",
    "        \n",
    "        unfolded = unfolded.transpose(1, 2).reshape(N, -1, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "\n",
    "        # Prepare unfolded for batch processing in coef2curve - Final shape becomes (C*K*K, N * L)\n",
    "        unfolded = unfolded.reshape(-1, self.in_channels * self.kernel_size * self.kernel_size).t()  # (batch_size*Hp*Wp, features)\n",
    "\n",
    "    \n",
    "        \n",
    "        # Output tensor initialization\n",
    "        Hp = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        Wp = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output = torch.zeros((N, self.out_channels, Hp, Wp), device=self.device)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Loop through each output channel\n",
    "        for c in range(self.out_channels):\n",
    "            # Calculate spline values for all pixels and sum in the first dimension (which has size kernel_size*kernel_size*in_channels)\n",
    "            spline_values = spline.coef2curve(unfolded, self.knots[c], self.coeff[c], self.degree, device=self.device).sum(dim=0)\n",
    "            output[:, c, :, :] = spline_values.view(N, Hp, Wp)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a13c973-58e9-4ad7-83f1-571a06591fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer CNN network with max pooling\n",
    "\n",
    "    Args:\n",
    "        in_channels: number of features of the input image (\"depth of image\")\n",
    "        hidden_channels: number of hidden features (\"depth of convolved images\")\n",
    "        out_features: number of features in output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_features):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "\n",
    "        self.conv1 = CKAN(in_channels, hidden_channels[0],\n",
    "                               kernel_size=3,\n",
    "                               padding=1, grid=13)\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels[0], hidden_channels[1],\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(28*28*in_channels, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        # Activation function\n",
    "        x = self.relu1(x)\n",
    "        # Max pool\n",
    "        x = self.max_pool1(x)\n",
    "        # Second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        # Activation function\n",
    "        x = self.relu2(x)\n",
    "        # Max pool\n",
    "        x = self.max_pool2(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "563e6c46-22f4-4c12-ad18-552b1dbceb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
    "# mean and dividing by std.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Get data from torchvision.datasets\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define data loaders used to iterate through dataset\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88fbaccb-a515-47a8-9170-9553f59c27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_loader, net, device=torch.device('cpu')):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "    net.eval()  #make sure network is in evaluation mode\n",
    "\n",
    "    #init\n",
    "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
    "    n = 0\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        # Copy the data to device.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
    "            n += y.shape[0] #increases with the number of samples in the batch\n",
    "    return acc_sum.item()/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20a22256-de20-4f55-b82b-56048f2f6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    \"\"\"\n",
    "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
    "    as cpu.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0703cb-c804-4385-8f49-fdaeaa2975c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-4.9449e-02,  2.7091e-02, -6.5744e-02, -1.3960e-01, -1.1842e-01,\n",
      "          -2.0481e-02,  8.0473e-02, -1.1531e-01, -1.4431e-02, -1.2936e-02,\n",
      "          -8.5938e-02,  8.5282e-02, -1.4052e-01, -6.7493e-02, -1.4931e-01,\n",
      "          -2.6079e-02],\n",
      "         [ 2.2896e-02,  5.2893e-02, -9.9172e-02, -1.7670e-01,  3.9778e-02,\n",
      "           1.9968e-01, -7.4233e-02,  5.9522e-02,  5.5490e-03, -1.1436e-01,\n",
      "           1.1908e-01, -2.3344e-02,  3.7260e-02,  4.7305e-03,  1.6983e-04,\n",
      "           6.8622e-03],\n",
      "         [ 1.0761e-01,  1.5855e-03,  5.7198e-02, -1.2630e-01, -4.7967e-02,\n",
      "          -2.4833e-01,  6.7611e-02,  2.2644e-02,  7.6594e-02,  3.4152e-02,\n",
      "          -1.6028e-01,  1.3409e-03, -1.4400e-01,  1.8635e-01,  8.4286e-02,\n",
      "          -7.1136e-02],\n",
      "         [-3.5804e-02, -7.4643e-02, -2.6075e-02,  5.1560e-02, -1.9013e-04,\n",
      "           1.3485e-01, -2.5301e-01, -4.6906e-02, -2.1204e-01,  6.0450e-03,\n",
      "           2.5866e-02,  6.1325e-02, -3.4185e-02, -7.5065e-02,  9.0991e-02,\n",
      "           1.3478e-02],\n",
      "         [-6.9297e-02, -7.2292e-02,  8.4286e-03, -1.1577e-01, -1.5838e-02,\n",
      "          -7.0919e-03, -1.3984e-01, -8.6235e-02, -3.8951e-02, -9.5191e-02,\n",
      "          -5.4253e-02,  3.2854e-02, -9.7475e-02,  2.2116e-01, -1.2216e-01,\n",
      "           4.4658e-02],\n",
      "         [ 4.5576e-02, -3.4278e-02,  1.6624e-02, -4.2555e-02,  7.9833e-02,\n",
      "           8.5349e-02,  4.3831e-02, -1.9903e-02,  3.8204e-02,  1.2248e-01,\n",
      "           1.4947e-01, -1.8963e-02, -5.3195e-02, -1.6395e-03,  1.4367e-02,\n",
      "           6.4792e-02],\n",
      "         [ 6.2537e-02, -3.3068e-02,  5.1916e-02, -2.1421e-01,  5.9614e-02,\n",
      "           3.0180e-01,  8.0028e-02, -1.5117e-02,  2.1622e-01, -1.3227e-01,\n",
      "           2.3131e-01,  6.4262e-02,  3.9817e-02,  2.9690e-01, -1.7027e-02,\n",
      "           9.0081e-02],\n",
      "         [ 4.0701e-02, -1.0475e-02,  7.2561e-02, -2.2594e-02, -8.6416e-03,\n",
      "          -1.4294e-01, -1.0955e-01, -6.5020e-03,  1.3686e-01, -1.6355e-01,\n",
      "           1.6215e-01, -1.2499e-01,  2.2530e-02, -1.0958e-01, -4.8059e-02,\n",
      "          -2.3608e-02],\n",
      "         [-9.1010e-02, -1.2818e-02, -4.6248e-02,  3.9066e-02, -8.3585e-02,\n",
      "          -4.9496e-02, -3.6822e-02, -5.3227e-02, -6.8616e-03,  2.9889e-01,\n",
      "           5.1181e-02, -2.0837e-02,  2.2841e-02, -5.9948e-02, -2.3599e-02,\n",
      "          -4.1758e-02]]], requires_grad=True)\n",
      "Epoch: 1\n",
      "Accuracy of train set: 28%\n",
      "Accuracy of test set: 29%\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[-0.0497,  0.0146, -0.0841, -0.1399, -0.1173, -0.0200,  0.0810,\n",
      "          -0.1146, -0.0138, -0.0122, -0.0852,  0.0859, -0.1399, -0.0669,\n",
      "          -0.1487, -0.0253],\n",
      "         [ 0.0226,  0.0358, -0.1243, -0.1776,  0.0408,  0.2019, -0.0724,\n",
      "           0.0603,  0.0062, -0.1136,  0.1198, -0.0224,  0.0382,  0.0055,\n",
      "           0.0008,  0.0076],\n",
      "         [ 0.1073, -0.0135,  0.0352, -0.1267, -0.0464, -0.2460,  0.0694,\n",
      "           0.0236,  0.0776,  0.0351, -0.1595,  0.0024, -0.1430,  0.1874,\n",
      "           0.0853, -0.0702],\n",
      "         [-0.0361, -0.0903, -0.0486,  0.0514,  0.0008,  0.1343, -0.2529,\n",
      "          -0.0460, -0.2108,  0.0073,  0.0267,  0.0623, -0.0334, -0.0744,\n",
      "           0.0916,  0.0143],\n",
      "         [-0.0697, -0.0920, -0.0204, -0.1167, -0.0146, -0.0060, -0.1388,\n",
      "          -0.0853, -0.0379, -0.0941, -0.0534,  0.0338, -0.0965,  0.2220,\n",
      "          -0.1215,  0.0456],\n",
      "         [ 0.0453, -0.0514, -0.0083, -0.0431,  0.0812,  0.0861,  0.0446,\n",
      "          -0.0189,  0.0392,  0.1234,  0.1504, -0.0180, -0.0521, -0.0005,\n",
      "           0.0154,  0.0657],\n",
      "         [ 0.0623, -0.0468,  0.0324, -0.2140,  0.0608,  0.3012,  0.0799,\n",
      "          -0.0143,  0.2171, -0.1313,  0.2320,  0.0650,  0.0405,  0.2973,\n",
      "          -0.0164,  0.0910],\n",
      "         [ 0.0404, -0.0268,  0.0488, -0.0233, -0.0078, -0.1418, -0.1086,\n",
      "          -0.0058,  0.1377, -0.1627,  0.1628, -0.1244,  0.0232, -0.1090,\n",
      "          -0.0476, -0.0230],\n",
      "         [-0.0913, -0.0262, -0.0657,  0.0383, -0.0828, -0.0488, -0.0362,\n",
      "          -0.0526, -0.0062,  0.2994,  0.0516, -0.0203,  0.0233, -0.0596,\n",
      "          -0.0231, -0.0411]]], requires_grad=True)\n",
      "Epoch: 2\n",
      "Accuracy of train set: 77%\n",
      "Accuracy of test set: 78%\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[[-0.0518, -0.1048, -0.2614, -0.1490, -0.1120, -0.0159,  0.0851,\n",
      "          -0.1102, -0.0094, -0.0079, -0.0811,  0.0904, -0.1356, -0.0626,\n",
      "          -0.1441, -0.0203],\n",
      "         [ 0.0203, -0.0942, -0.3173, -0.1881,  0.0460,  0.2058, -0.0682,\n",
      "           0.0648,  0.0108, -0.1090,  0.1241, -0.0176,  0.0429,  0.0101,\n",
      "           0.0055,  0.0127],\n",
      "         [ 0.1052, -0.1329, -0.1420, -0.1358, -0.0409, -0.2411,  0.0742,\n",
      "           0.0281,  0.0822,  0.0396, -0.1554,  0.0070, -0.1385,  0.1918,\n",
      "           0.0900, -0.0652],\n",
      "         [-0.0385, -0.2252, -0.2483,  0.0414,  0.0066,  0.1399, -0.2476,\n",
      "          -0.0410, -0.2055,  0.0126,  0.0314,  0.0675, -0.0285, -0.0697,\n",
      "           0.0966,  0.0200],\n",
      "         [-0.0722, -0.2331, -0.2298, -0.1277, -0.0086, -0.0006, -0.1336,\n",
      "          -0.0803, -0.0327, -0.0889, -0.0487,  0.0391, -0.0915,  0.2269,\n",
      "          -0.1164,  0.0514],\n",
      "         [ 0.0430, -0.1773, -0.1950, -0.0525,  0.0869,  0.0918,  0.0498,\n",
      "          -0.0143,  0.0441,  0.1282,  0.1548, -0.0131, -0.0473,  0.0044,\n",
      "           0.0203,  0.0711],\n",
      "         [ 0.0601, -0.1666, -0.1448, -0.2223,  0.0665,  0.3059,  0.0845,\n",
      "          -0.0098,  0.2218, -0.1267,  0.2362,  0.0696,  0.0448,  0.3015,\n",
      "          -0.0119,  0.0960],\n",
      "         [ 0.0382, -0.1477, -0.1302, -0.0325, -0.0026, -0.1371, -0.1041,\n",
      "          -0.0015,  0.1424, -0.1581,  0.1668, -0.1199,  0.0276, -0.1047,\n",
      "          -0.0432, -0.0181],\n",
      "         [-0.0931, -0.1296, -0.2188,  0.0307, -0.0780, -0.0439, -0.0317,\n",
      "          -0.0487, -0.0021,  0.3032,  0.0551, -0.0165,  0.0270, -0.0560,\n",
      "          -0.0192, -0.0367]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "in_channels = 1 # Black-white images in MNIST digits\n",
    "hidden_channels = [1, 6]\n",
    "out_features = 10\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.005\n",
    "epochs = 5\n",
    "\n",
    "# Initialize network\n",
    "net = Net(in_channels, hidden_channels, out_features)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define list to store losses and performances of each iteration\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "\n",
    "\n",
    "# Try using gpu instead of cpu\n",
    "device = try_gpu()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Network in training mode and to device\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "    print(net.conv1.coeff)\n",
    "\n",
    "    # Training loop\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Set to same device\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Perform forward pass\n",
    "        y_pred = net(x_batch)\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # Backward computation and update\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute train and test error\n",
    "    train_acc = 100*evaluate_accuracy(train_loader, net.to('cpu'))\n",
    "    test_acc = 100*evaluate_accuracy(test_loader, net.to('cpu'))\n",
    "\n",
    "    # Development of performance\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    # Print performance\n",
    "    print('Epoch: {:.0f}'.format(epoch+1))\n",
    "    print('Accuracy of train set: {:.00f}%'.format(train_acc))\n",
    "    print('Accuracy of test set: {:.00f}%'.format(test_acc))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06ae9d-cceb-48a1-8978-6b15e11cd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'train_losses' is a list of tensors\n",
    "train_losses_detached = [loss.detach().numpy() for loss in train_losses]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses_detached)  # Use the detached list of numpy arrays\n",
    "plt.grid()\n",
    "\n",
    "# Assuming you have additional plots or other code to follow\n",
    "plt.subplot(1,2,2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(train_accs, label = 'train')\n",
    "plt.plot(test_accs, label = 'test')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
